Automatically generated by Mendeley Desktop 1.14
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@inproceedings{Turian2010,
abstract = {If we take an existing supervised \{NLP\} system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and \{HLBL\} (Mnih \& Hinton, 2009) embeddings of words on both \{NER\} and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing \{NLP\} systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/},
author = {Turian, Joseph and Ratinov, Lev and Bengio, Yoshua},
booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
pages = {384--394},
publisher = {Association for Computational Linguistics},
series = {ACL '10},
shorttitle = {Word Representations},
title = {{Word Representations: A Simple and General Method for Semi-supervised Learning}},
url = {http://dl.acm.org/citation.cfm?id=1858721$\backslash$nhttp://dl.acm.org/citation.cfm?id=1858681.1858721},
year = {2010}
}
@inproceedings{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {NIPS},
pages = {1--9},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{Fu2014,
abstract = {Semantic hierarchy construction aims to build structures of concepts linked by hypernym–hyponym (“is-a”) relations. A major challenge for this task is the automatic discovery of such relations. This paper proposes a novel and effec- tive method for the construction of se- mantic hierarchies based on word em- beddings, which can be used to mea- sure the semantic relationship between words. We identify whether a candidate word pair has hypernym–hyponym rela- tion by using the word-embedding-based semantic projections between words and their hypernyms. Our result, an F-score of 73.74\%, outperforms the state-of-the- art methods on a manually labeled test dataset. Moreover, combining our method with a previous manually-built hierarchy extension method can further improve F- score to 80.29\%. 1},
author = {Fu, Ruiji and Guo, Jiang and Qin, Bing and Che, Wanxiang and Wang, Haifeng and Liu, Ting},
journal = {ACL},
pages = {1199--1209},
title = {{Learning Semantic Hierarchies via Word Embeddings}},
volume = {Proceeding},
year = {2014}
}
@book{Fellbaum1998,
abstract = {WordNet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets. The purpose of this volume is twofold. First, it discusses the design of WordNet and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains.},
author = {Fellbaum, Christiane},
booktitle = {British Journal Of Hospital Medicine London England 2005},
chapter = {Combining},
editor = {Fellbaum, Christiane},
number = {3},
pages = {423},
publisher = {MIT Press},
series = {Language, Speech, and Communication},
title = {{WordNet: An Electronic Lexical Database}},
url = {http://acl.ldc.upenn.edu/J/J99/J99-2008.pdf},
volume = {71},
year = {1998}
}
@article{Hong,
author = {Hong, James and Fang, Michael},
file = {:Users/lukas/Library/Application Support/Mendeley Desktop/Downloaded/Hong, Fang - Unknown - Sentiment Analysis with Deeply Learned Distributed Representations of Variable Length Texts.pdf:pdf},
journal = {cs224d.stanford.edu},
title = {{Sentiment Analysis with Deeply Learned Distributed Representations of Variable Length Texts}},
url = {http://cs224d.stanford.edu/reports/HongJames.pdf},
year = {2015}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Quoc V. and Mikolov, Tomas},
eprint = {1405.4053},
journal = {CoRR},
month = may,
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {abs/1405.4},
year = {2014}
}
@article{Harris1954,
abstract = {Harris maintains that it is possible to define a linguistic structure solely in terms of the "distributions" (= patterns of co-occurrences) of its elements. There is no parallel meaning-structure which can aid in describing formal structure. Meaning is partly a function of distribution.},
author = {Harris, Zellig S.},
file = {:Users/lukas/Library/Application Support/Mendeley Desktop/Downloaded/Harris - 1954 - Distributional structure.pdf:pdf},
journal = {Word},
number = {23},
pages = {146--162},
title = {{Distributional structure}},
volume = {10},
year = {1954}
}
@misc{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
booktitle = {Nature},
number = {6088},
pages = {533--536},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@inproceedings{Luong2013,
abstract = {Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologically-aware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way.},
author = {Luong, Minh-thang and Manning, Christopher D},
booktitle = {Conference on Computational Natural Language Learning (CoNNL 2013)},
title = {{Better Word Representations with Recursive Neural Networks for Morphology}},
year = {2013}
}
@article{Bengio2006,
abstract = {A central goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on several methods to speed-up both training and probability computation, as well as comparative experiments to evaluate the improvements brought by these techniques. We finally describe the incorporation of this new language model into a state-of-the-art speech recognizer of conversational speech.},
author = {Bengio, Yoshua and Schwenk, Holger and Sen\'{e}cal, Jean S\'{e}bastien and Morin, Fr\'{e}deric and Gauvain, Jean Luc},
journal = {Studies in Fuzziness and Soft Computing},
pages = {137--186},
title = {{Neural probabilistic language models}},
volume = {194},
year = {2006}
}
@inproceedings{Maas2011,
abstract = {Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term–document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment in- formation as well as non-sentiment annota- tions. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.},
author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
pages = {142--150},
title = {{Learning Word Vectors for Sentiment Analysis}},
year = {2011}
}
@article{Mikolov2013b,
abstract = {Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90\% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.},
archivePrefix = {arXiv},
arxivId = {1309.4168},
author = {Mikolov, Tomas and Le, Quoc V. and Sutskever, Ilya},
eprint = {1309.4168},
file = {:Users/lukas/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov, Le, Sutskever - 2013 - Exploiting Similarities among Languages for Machine Translation.pdf:pdf},
journal = {CoRR},
title = {{Exploiting Similarities among Languages for Machine Translation}},
url = {http://arxiv.org/abs/1309.4168v1$\backslash$nhttp://arxiv.org/abs/1309.4168},
volume = {abs/1309.4},
year = {2013}
}
@article{Goldberg2014a,
abstract = {The word2vec software of Tomas Mikolov and colleagues1 has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers [1, 2]. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in “Dis- tributed Representations ofWords and Phrases and their Compositionality” by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean [2].},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.3722v1},
author = {Goldberg, Yoav and Levy, Omer},
eprint = {arXiv:1402.3722v1},
journal = {CoRR},
title = {{word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method}},
url = {http://arxiv.org/abs/1402.3722},
volume = {abs/1402.3},
year = {2014}
}
@inproceedings{rehurek_lrec,
abstract = {Large corpora are ubiquitous in today’s world and memory quickly becomes the limiting factor in practical applications of the Vector Space Model (VSM). In this paper, we identify a gap in existing implementations of many of the popular algorithms, which is their scalability and ease of use. We describe a Natural Language Processing software framework which is based on the idea of document streaming, i.e. processing corpora document after document, in a memory independent fashion. Within this framework, we implement several popular algorithms for topical inference, including Latent Semantic Analysis and Latent Dirichlet Allocation, in a way that makes them completely independent of the training corpus size. Particular emphasis is placed on straightforward and intuitive framework design, so that modifications and extensions of the methods and/or their application by interested practitioners are effortless. We demonstrate the usefulness of our approach on a real-world scenario of computing document similarities within an existing digital library DML-CZ.},
address = {Valletta, Malta},
annote = {$\backslash$url\{http://is.muni.cz/publication/884893/en\}},
author = {Řehůřek, Radim and Sojka, Petr},
booktitle = {Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks},
isbn = {2-9517408-6-7},
issn = {2951740867},
month = may,
pages = {45--50},
publisher = {ELRA},
title = {{Software framework for topic modelling with large corpora}},
url = {http://www.muni.cz/research/publications/884893},
year = {2010}
}
@article{Rong2014,
author = {Rong, Xin},
journal = {CoRR},
title = {{word2vec Parameter Learning Explained}},
url = {http://arxiv.org/abs/1411.2738},
volume = {abs/1411.2},
year = {2014}
}
@article{Mnih2009,
author = {Mnih, Aandriy and Hinton, Geoffrey E.},
editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
journal = {Advances in Neural Information Processing Systems 21},
pages = {1081--1088},
title = {{A Scalable Hierarchical Distributed Language Model}},
url = {http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model},
year = {2009}
}
@inproceedings{LeCun1985,
abstract = {Important early paper on backpropagation. Examines a learning scheme that is roughly similar to the generalized delta rule derived by Rumelhart, Hinton \& Williams (1986).},
author = {LeCun, Yann},
booktitle = {Proceedings of Cognitiva 85},
keywords = {Connectionism Backpropagation},
pages = {599--604 ST -- Une procedure d'apprentissage pour r},
title = {{Une procedure d'apprentissage pour reseau a seuil assymetrique (A learning procedure for assymetric threshold networks)}},
year = {1985}
}
@article{Shi2014,
abstract = {The Global Vectors for word representation (GloVe), introduced by Jeffrey Pennington et al. is reported to be an efficient and effective method for learning vector representations of words. State-of-the-art performance is also provided by skip-gram with negative-sampling (SGNS) implemented in the word2vec tool. In this note, we explain the similarities between the training objectives of the two models, and show that the objective of SGNS is similar to the objective of a specialized form of GloVe, though their cost functions are defined differently.},
archivePrefix = {arXiv},
arxivId = {1411.5595},
author = {Shi, Tianze and Liu, Zhiyuan},
eprint = {1411.5595},
journal = {CoRR},
month = nov,
pages = {5},
title = {{Linking GloVe with word2vec}},
url = {http://arxiv.org/abs/1411.5595},
volume = {abs/1411.5},
year = {2014}
}
@article{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
journal = {The Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@incollection{Bengio2007,
abstract = {One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), rea- soning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, withmin- imal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally lim- ited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very ineffi- cient in terms of required number of computational elements and examples. Sec- ond, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learn- ing) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more ab- stract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence. 1},
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
chapter = {14},
editor = {Bottou, L\'{e}on and Chapelle, Olivier and DeCoste, D and Weston, J},
file = {:Users/lukas/Library/Application Support/Mendeley Desktop/Downloaded/Bengio, \{LeCun\} - 2007 - Scaling Learning Algorithms towards AI.pdf:pdf},
number = {1},
pages = {321--360},
publisher = {MIT Press},
title = {{Scaling Learning Algorithms towards AI}},
year = {2007}
}
@article{Wolf2013,
abstract = {We extend the word2vec framework to capture meaning across languages. The input consists of a source text and a word-aligned parallel text in a second language. The joint word2vec tool then represents words in both languages within a common “semantic” vector space. The result can be used to enrich lexicons of under-resourced languages, to identify ambiguities, and to perform clustering and classification. Experiments were conducted on a parallel English-Arabic corpus, as well as on English and Hebrew Biblical texts.},
author = {Wolf, Lior and Hanani, Yair and Bar, Kfir and Dershowitz, Nachum},
journal = {International Journal of Computational Linguistics and Applications},
pages = {27--44},
title = {{Joint word2vec Networks for Bilingual Semantic Representations}},
url = {http://www.cs.tau.ac.il/~nachumd/papers/jw2v.pdf},
volume = {5.1},
year = {2014}
}
@inproceedings{Morin2005,
abstract = {In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.},
author = {Morin, Frederic and Bengio, Y},
booktitle = {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
pages = {246--252},
title = {{Hierarchical probabilistic neural network language model}},
url = {http://www.iro.umontreal.ca/labs/neuro/pointeurs/hierarchical-nnlm-aistats05.pdf$\backslash$nhttp://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf},
year = {2005}
}
@article{Jameel2013,
abstract = {We present a new unsupervised topic discovery model for a collection of text documents. In contrast to the majority of the state-of-the-art topic models, our model does not break the document's structure such as paragraphs and sentences. In addition, it preserves word order in the document. As a result, it can generate two levels of topics of different granularity, namely, segment-topics and word-topics. In addition, it can generate n-gram words in each topic. We also develop an approximate inference scheme using Gibbs sampling method. We conduct extensive experiments using publicly available data from different collections and show that our model improves the quality of several text mining tasks such as the ability to support fine grained topics with n-gram words in the correlation graph, the ability to segment a document into topically coherent sections, document classification, and document likelihood estimation.},
author = {Jameel, Shoaib and Lam, Wai},
journal = {Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval - SIGIR '13},
keywords = {document classification,gibbs,n-gram words,sampling,topic modeling,topic segmentation},
pages = {203},
publisher = {ACM Press},
title = {{An unsupervised topic segmentation model incorporating word order}},
url = {http://dl.acm.org/citation.cfm?doid=2484028.2484062},
year = {2013}
}
@inproceedings{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75\% accuracy, an improvement of 11\% over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
publisher = {Association for Computational Linguistics},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}
@article{Kombrink2011,
author = {Kombrink, Stefan and Mikolov, Tomas and Karafi\'{a}t, Martin and Burget, Lukas},
file = {:Users/lukas/Library/Application Support/Mendeley Desktop/Downloaded/Kombrink et al. - 2011 - Recurrent Neural Network Based Language Modeling in Meeting Recognition.pdf:pdf},
journal = {INTERSPEECH},
title = {{Recurrent Neural Network Based Language Modeling in Meeting Recognition.}},
url = {http://www.researchgate.net/profile/Stefan\_Kombrink/publication/221484390\_Recurrent\_Neural\_Network\_Based\_Language\_Modeling\_in\_Meeting\_Recognition/links/00b4952c265a22006e000000.pdf},
year = {2011}
}
@inproceedings{Collobert2008,
abstract = {We describe a single convolutional neural net- work architecture that, given a sentence, out- puts a host of language processing predic- tions: part-of-speech tags, chunks, named en- tity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semanti- cally) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data ex- cept the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the general- ization of the shared tasks, resulting in state- of-the-art performance.},
author = {Collobert, Ronan and Weston, Jason},
booktitle = {Machine Learning, Proceedings of the Twenty-Fifth International Conference},
doi = {10.1145/1390156.1390177},
isbn = {9781605582054},
issn = {07224028},
organization = {ACM},
pages = {160--167},
publisher = {Acm},
series = {ICML '08},
title = {{A Unified Architecture for Natural Language Processing : Deep Neural Networks with Multitask Learning}},
url = {http://portal.acm.org/citation.cfm?id=1390177},
year = {2008}
}
@article{Wu2008,
author = {Wu, Ho Chung and Luk, Robert Wing Pong and Wong, Kam Fai and Kwok, Kui Lam},
doi = {10.1145/1361684.1361686},
issn = {10468188},
journal = {ACM Transactions on Information Systems},
keywords = {Information retrieval,relevance decision,term weight},
month = jun,
number = {3},
pages = {1--37},
publisher = {ACM},
title = {{Interpreting TF-IDF term weights as making relevance decisions}},
url = {http://dl.acm.org/citation.cfm?id=1361684.1361686},
volume = {26},
year = {2008}
}
@article{Rajaraman2011,
abstract = {At the highest level of description, this book is about data mining. However, it focuses on data mining of very large amounts of data, that is, data so large it does not fit in main memory. Because of the emphasis on size, many of our examples are about the Web or data derived from the Web. Further, the book takes an algorithmic point of view: data mining is about applying algorithms to data, rather than using data to train a machine-learning engine of some sort.},
author = {Rajaraman, Anand and Ullman, Jeffrey D.},
journal = {Lecture Notes for Stanford CS345A Web Mining},
pages = {328},
title = {{Mining of Massive Datasets}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139058452},
volume = {67},
year = {2011}
}
@inproceedings{Mikolov2013a,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
year = {2013}
}
@inproceedings{TomasMikolov,
abstract = {Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words.},
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
booktitle = {HLT-NAACL},
pages = {746--751},
title = {{Linguistic regularities in continuous space word representations}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.417.722},
year = {2013}
}
@article{Miller1990,
abstract = {WordNet is an on-line lexical reference system whose design is inspired by current},
author = {Miller, George},
journal = {International Journal of Lexicography},
pages = {235--244},
title = {{WordNet: An on-line lexical database}},
url = {ftp://ftp.cogsci.princeton.edu/pub/wordnet/5papers.pdf},
volume = {3},
year = {1990}
}
@article{Ackley1985,
abstract = {Presents a learning algorithm for the Boltzmann Machine, a parallel constraint satisfaction network that is capable of learning the underlying constraints that characterize a domain simply by being shown examples from the domain. The Boltzmann Machine formulation leads to a domain-independent general learning algorithm for modifying the connection strengths so as to incorporate knowledge about a task domain so that the whole network develops an internal model that captures the underlying structure of its environment. Simple examples are provided, in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure. (28 ref) (PsycINFO Database Record (c) 2003 APA )},
author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
doi = {10.1016/S0364-0213(85)80012-4},
isbn = {0364-0213},
issn = {0364-0213},
journal = {Cognitive Science. Special Issue: Connectionist models and their applications},
keywords = {*Algorithms Artificial Intelligence Learning},
number = {1},
pages = {147--169 ST -- A learning algorithm for Boltzmann M},
title = {{A learning algorithm for Boltzmann Machines}},
url = {http://www.elsevier.com},
volume = {9},
year = {1985}
}
